{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0634b256",
   "metadata": {},
   "source": [
    "In this example we'll do hyper-parameter tuning and search with XGB and Bayes Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8638c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import plot_tree\n",
    "from bayes_opt import BayesianOptimization\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "             "
   ]
  },
  {
   "cell_type": "raw",
   "id": "122fb693",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a55ccaee",
   "metadata": {},
   "source": [
    "XGB and Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4fed8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_excel('Lotto5.xlsx')\n",
    "\n",
    "# Select the input features\n",
    "features = data[['Draw', 'Odd', 'Even', '1-10', '11-20', '21-30', '31-40', 'Division 1 Winners', 'Division 2 Winners']]\n",
    "\n",
    "# Impute missing values using KNN imputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "features_imputed = imputer.fit_transform(features)\n",
    "\n",
    "# Train the model\n",
    "target = data[['Winning Number 1', 'Winning Number 2', 'Winning Number 3', 'Winning Number 4', 'Winning Number 5', 'Winning Number 6', 'Bonus Number']]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_imputed, X_test_imputed, y_train, y_test = train_test_split(features_imputed, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the XGBoost model with an evaluation set\n",
    "model = XGBRegressor(n_estimators=1000, learning_rate=0.05, random_state=42)\n",
    "model.fit(X_train_imputed, y_train, eval_set=[(X_test_imputed, y_test)], eval_metric='rmse', early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "# Define the objective function for Bayesian optimization\n",
    "def xgb_cv(max_depth, gamma, colsample_bytree):\n",
    "    params = {'n_estimators': 1000,\n",
    "              'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'random_state': 42}\n",
    "    xgb_model = xgb.XGBRegressor(**params)\n",
    "    neg_mse = -np.mean(cross_val_score(xgb_model, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error'))\n",
    "    return neg_mse\n",
    "\n",
    "# Define the search space for hyper-parameters\n",
    "xgb_bo = BayesianOptimization(xgb_cv,\n",
    "                              {'max_depth': (3, 10),\n",
    "                               'gamma': (0, 1),\n",
    "                               'colsample_bytree': (0.3, 1)})\n",
    "\n",
    "# Perform Bayesian Optimization to find the best hyper-parameters\n",
    "xgb_bo.maximize(n_iter=10, init_points=3, acq='ei')\n",
    "\n",
    "# Train the model using the optimized hyper-parameters\n",
    "params = xgb_bo.max['params']\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "model = xgb.XGBRegressor(n_estimators=1000, random_state=42, **params)\n",
    "model.fit(X_train_imputed, y_train, early_stopping_rounds=10, eval_set=[(X_train_imputed, y_train), (X_test_imputed, y_test)], eval_metric='rmse', verbose=False)\n",
    "\n",
    "# Visualize the feature importance\n",
    "plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "plt.xticks(range(len(model.feature_importances_)), features.columns, rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the training history\n",
    "plt.plot(model.evals_result()['validation_0']['rmse'], label='Train')\n",
    "plt.plot(model.evals_result()['validation_1']['rmse'], label='Test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "\n",
    "# Perform cross-validation to evaluate model performance\n",
    "scores = cross_val_score(model, features_imputed, target, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Cross-validation scores: {-scores}\")\n",
    "print(f\"Mean cross-validation score: {-np.mean(scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40dba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple sets of predicted numbers\n",
    "num_predictions = 5\n",
    "for i in range(num_predictions):\n",
    "    input_data = np.random.randint(1, 50, size=(1, len(features.columns)))\n",
    "    prediction = model.predict(imputer.transform(input_data))\n",
    "    predicted_numbers = np.round((prediction.flatten() % 40) + 1)\n",
    "    print(f\"Predicted winning numbers for set {i+1}: {predicted_numbers}\")\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "from hyperopt import fmin, hp, tpe\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(1, 20)),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0.01, 0.5, 0.01),\n",
    "    'n_estimators': hp.choice('n_estimators', range(50, 1000)),\n",
    "    'subsample': hp.quniform('subsample', 0.1, 1, 0.01),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.01),\n",
    "    'reg_alpha': hp.quniform('reg_alpha', 0, 1, 0.01),\n",
    "    'reg_lambda': hp.quniform('reg_lambda', 0, 1, 0.01),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1)\n",
    "}\n",
    "\n",
    "# Define the objective function for hyperparameter optimization\n",
    "def objective_function(params):\n",
    "    model = xgb.XGBRegressor(\n",
    "        max_depth=int(params['max_depth']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        subsample=params['subsample'],\n",
    "        gamma=params['gamma'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        min_child_weight=int(params['min_child_weight']),\n",
    "        random_state=42\n",
    "    )\n",
    "    scores = cross_val_score(model, features_imputed, target, cv=5, scoring='neg_mean_squared_error')\n",
    "    rmse = np.sqrt(-np.mean(scores))\n",
    "    return rmse\n",
    "\n",
    "# Run the hyperparameter search using Bayesian optimization\n",
    "best = fmin(fn=objective_function, space=space, algo=tpe.suggest, max_evals=100)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = xgb.XGBRegressor(\n",
    "    max_depth=int(best['max_depth']),\n",
    "    learning_rate=best['learning_rate'],\n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    subsample=best['subsample'],\n",
    "    gamma=best['gamma'],\n",
    "    reg_alpha=best['reg_alpha'],\n",
    "    reg_lambda=best['reg_lambda'],\n",
    "    min_child_weight=int(best['min_child_weight']),\n",
    "    random_state=42\n",
    ")\n",
    "best_model.fit(features_imputed, target, early_stopping_rounds=10, eval_set=[(features_imputed, target)], verbose=False)\n",
    "\n",
    "# Visualize the feature importance for the best model\n",
    "plt.bar(range(len(best_model.feature_importances_)), best_model.feature_importances_)\n",
    "plt.xticks(range(len(best_model.feature_importances_)), features.columns, rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance - Best Model')\n",
    "plt.show()\n",
    "\n",
    "#Visualize the training history for the best model\n",
    "plt.plot(best_model.evals_result()['validation_0']['rmse'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training History - Best Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7b332",
   "metadata": {},
   "source": [
    "Evaluate the best model performance and generate 5 sets of results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation to evaluate the performance of the best model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "scoring = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "cv_results = cross_validate(model, features_imputed, target, cv=5, scoring=scoring, return_estimator=True)\n",
    "\n",
    "best_model_index = np.argmin(cv_results['test_score'])\n",
    "best_model = cv_results['estimator'][best_model_index]\n",
    "best_score = -cv_results['test_score'][best_model_index]\n",
    "\n",
    "print(f\"Cross-validation scores: {-cv_results['test_score']}\")\n",
    "print(f\"Mean cross-validation score: {-np.mean(cv_results['test_score'])}\")\n",
    "print(f\"Best cross-validation score: {best_score}\")\n",
    "\n",
    "\n",
    "# Generate multiple sets of predicted numbers using the best model\n",
    "num_predictions = 5\n",
    "for i in range(num_predictions):\n",
    "    input_data = np.random.randint(1, 50, size=(1, len(features.columns)))\n",
    "    prediction = best_model.predict(imputer.transform(input_data))\n",
    "    predicted_numbers = np.round((prediction.flatten() % 40) + 1)\n",
    "    print(f\"Predicted winning numbers for set {i+1}: {predicted_numbers}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230869fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cb8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b32e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b7772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aef832fb882e4402c23925109a7436b47557f1f6908741040efdc3941ef6e528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
